# Project List

## Webscraping and automating with GitHub actions

```{dropdown} What you will learn
1. HTML and CSS: Web scraping involves extracting data from websites, which are written in HTML and CSS. Understanding the structure and elements of HTML and CSS is crucial for being able to locate and extract the desired data from a webpage.
2. HTTP and web protocols: Web scraping requires making HTTP requests to a website and understanding the web protocols such as GET, POST and handling cookies and sessions.
3. XPath and CSS selectors: To extract data from a webpage, you'll need to be able to locate the relevant elements on the page. This can be done using XPath and CSS selectors, which are used to navigate and select elements in an HTML or XML document.
4. Web scraping libraries: Python offers a variety of libraries for web scraping such as Scrapy, Beautiful Soup, Selenium among others, you will learn how to use them to extract data from websites in a fast and efficient way.
5. Data processing and data structures: After extracting data from a website, you will need to process and structure the data. This may involve cleaning, normalizing and transforming the data so it can be used for further analysis.
6. Web scraping best practices: Web scraping can be against the website's term of service and can also cause issues on the website's performance, so it's important to learn about best practices like respecting robots.txt, using APIs when possible and being mindful of the scraping frequency.
7.Ethics and legal considerations: Web scraping can raise legal and ethical issues, it's important to learn the legal framework and ethical considerations around web scraping.

By learning Python web scraping, you will gain the skills and knowledge needed to extract data from websites and use it to drive business decisions, research, and more.
```

```{dropdown} Webscraping libraries
- **Scrapy:** It is an open-source and collaborative web crawling framework for Python. Scrapy allows you to extract data from websites using spiders, which are scripts that navigate through a website and extract the desired information. Scrapy also provides a built-in way to follow links, handle cookies and sessions, and handle pagination, making it a powerful tool for web scraping.
- **Beautiful Soup:** This is a Python library that allows you to parse HTML and XML documents, and extract data from them. Beautiful Soup is not as robust as Scrapy in terms of handling sessions and pagination but it is simpler to use and is good for small web scraping projects, it's also helpful when you want to extract data from a single page.
Both libraries are free and open-source, and have an active community that provides support and tutorials.

It's worth noting that web scraping can be against the website's term of service and can also cause issues on the website's performance, it's important to check the website's policy and also use the scraping tools in a responsible way.
```

[Automating with GitHub actions](https://www.swyx.io/github-scraping))

### Data Transformation

1. Create EER diagrams of data model in PosgreSQL.